import csv
import os
import cv2
import torch
import numpy as np
import pandas as pd
from typing import List
from pydantic import BaseModel


class Row(BaseModel):
    image_name: str
    scroll_number: int
    xmin: float
    ymin: float
    xmax: float
    ymax: float
    iou: float = -1
    

def correct_bounding_boxes(bboxes):
    # Ensure input is a NumPy array
    bboxes = np.asarray(bboxes)
    
    # Correct x coordinates
    # Swap the x coordinates if the top-left x is greater than the bottom-right x
    x_min = np.minimum(bboxes[:, 0], bboxes[:, 2])
    x_max = np.maximum(bboxes[:, 0], bboxes[:, 2])
    
    # Correct y coordinates
    # Swap the y coordinates if the top-left y is greater than the bottom-right y
    y_min = np.minimum(bboxes[:, 1], bboxes[:, 3])
    y_max = np.maximum(bboxes[:, 1], bboxes[:, 3])
    
    # Construct the corrected bounding boxes array
    corrected_bboxes = np.stack([x_min, y_min, x_max, y_max], axis=1)
    
    return corrected_bboxes


def sort_by_distance(bbox_list): 
    # Compute distances from (0,0) using the top-left point (x_min, y_min)
    distances = torch.sqrt(bbox_list[:, 0]**2 + bbox_list[:, 1]**2)

    # Sort indices based on distance
    sorted_indices = torch.argsort(distances)

    # Sort the tensor
    sorted_tensor = bbox_list[sorted_indices]

    return sorted_tensor


def export_training_results_to_csv(train_result: List[Row], csv_file):
    with open(csv_file, mode="w", newline="") as file:
        writer = csv.DictWriter(file, fieldnames=train_result[0].model_dump().keys())
        writer.writeheader()   
        for res in train_result:
            writer.writerow(res.model_dump())  
    
    print(f"CSV file '{csv_file}' has been created successfully.")



def draw_bounding_boxes_from_csv(image_dir_path, csv_path):
    output_folder = "output_images/"
    os.makedirs(output_folder, exist_ok=True)

    image_name = None
    
    df = pd.read_csv(csv_path)

    grouped = df.groupby('image_name')
    
    for image_name, group in grouped:  
        image_name += ".jpg"
        image_path = os.path.join(image_dir_path, image_name)
        image = cv2.imread(image_path)
    
        for _, row in group.iterrows():
            xmin, ymin, xmax, ymax = map(int, [row['xmin'], row['ymin'], row['xmax'], row['ymax']])
            confidence = row.get('iou', -1)
            label = row.get('scroll_number', "scroll")

            # Draw rectangle
            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)

            # Draw label and confidence
            text = f"{label}: {confidence:.2f}"
            cv2.putText(image, text, (xmin, max(ymin - 10, 20)), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 3)  # Bigger font
        
        output_path = os.path.join(output_folder, image_name)    
        cv2.imwrite(output_path, image)
        print(f"Saved: {output_path}")




def draw_bounding_boxes_from_csv_single_image(image_path, csv_path):
    output_folder = "output_images/"
    os.makedirs(output_folder, exist_ok=True)

    image_name: str = os.path.splitext(os.path.basename(image_path))[0]
    
    df = pd.read_csv(csv_path)
    image = cv2.imread(image_path)

    for _, row in df.iterrows():
        xmin, ymin, xmax, ymax = map(int, [row['xmin'], row['ymin'], row['xmax'], row['ymax']])
        confidence = row.get('iou', -1)
        label = row.get('scroll_number', "scroll")

        # Draw rectangle
        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)

        # Draw label and confidence
        text = f"{label}: {confidence:.2f}"
        cv2.putText(image, text, (xmin, max(ymin - 10, 20)), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 3)  # Bigger font
    
    image_name += ".JPG"
    output_path = os.path.join(output_folder, image_name)
    cv2.imwrite(output_path, image)
    print(f"Saved: {output_path}")

    

def calculate_precision(iou_scores, threshold=0.5):
    TP = sum(1 for iou in iou_scores if iou >= threshold)  
    FP = sum(1 for iou in iou_scores if iou < threshold and iou > 0) 
    precision = TP / (TP + FP) if (TP + FP) > 0 else 0
    return precision

def calculate_map(iou_scores, thresholds=[0.5, 0.75, 0.9]):
    precisions = [calculate_precision(iou_scores, t) for t in thresholds]
    map_value = sum(precisions) / len(thresholds) 
    return map_value

def calculate_recall(iou_scores, threshold=0.5):
    TP = sum(1 for iou in iou_scores if iou >= threshold) 
    FN = sum(1 for iou in iou_scores if iou == 0.0)
    recall = TP / (TP + FN) if (TP + FN) > 0 else 0
    
    return recall

def calculate_f1(precision, recall):
    return (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0



# iou_scores = [0.9181267619132996, 0.8027963638305664, 0.7509658932685852, 0.9194065928459167, 0.855832576751709, 0.8450172543525696, 0.9053326845169067, 0.8564311265945435, 0.7165610194206238, 0.8841506838798523, 0.9168248772621155, 0.7523345351219177, 0.8202004432678223, 0.8502155542373657, 0.7798286080360413, 0.6645280718803406, 0.893202006816864, 0.8979058861732483, 0.8841734528541565, 0.8986249566078186, 0.9514345526695251, 0.9237368106842041, 0.9314088225364685, 0.678463339805603, 0.5467933416366577, 0.8567487597465515, 0.862877607345581, 0.6020240783691406, 0.8645217418670654, 0.816780149936676, 0.7884654998779297, 0.7959508299827576, 0.8646968603134155, 0.9391522407531738, 0.9458153247833252, 0.8620190620422363, 0.8484771251678467, 0.9204944372177124, 0.9141135215759277, 0.6704838871955872, 0.858488142490387, 0.6690470576286316, 0.769018828868866, 0.853803277015686, 0.9354599714279175, 0.9030565619468689, 0.9259775280952454, 0.7493363618850708, 0.7656900882720947, 0.8969818949699402, 0.8152084946632385, 0.8023607134819031, 0.8143582344055176, 0.8615309000015259, 0.8666566014289856, 0.8386030793190002, 0.6437219381332397, 0.9213824272155762, 0.8140357136726379, 0.8723442554473877, 0.6735416650772095, 0.807309627532959, 0.7667243480682373, 0.17103224992752075, 0.7505435347557068, 0.8408178687095642, 0.8355554342269897, 0.6835280060768127, 0.7826513051986694, 0.8303455710411072, 0.8903632164001465, 0.9599249958992004, 0.7882018089294434, 0.8613409996032715, 0.775094211101532, 0.9560069441795349, 0.7866708636283875, 0.8399757742881775, 0.8197546601295471, 0.7478599548339844, 0.8330608010292053, 0.9183003902435303, 0.9268032908439636, 0.06241891160607338, 0.02487274631857872, 0.07258202880620956, 0.8685159087181091, 0.6419405341148376, 0.30458372831344604, 0.7273932099342346, 0.8421536087989807, 0.8003730773925781, 0.8518738150596619, 0.8391945958137512, 0.7613105773925781, 0.5795533657073975, 0.7925201654434204, 0.841427206993103, 0.2677287757396698, 0.8040724992752075, 0.1254008263349533, 0.4173596203327179, 0.6765841841697693, 0.01386048924177885, 0.8033843636512756, 0.5288046002388, 0.4624011218547821, 0.7542172074317932, 0.03321561962366104, 0.6113936901092529, 0.30182239413261414, 0.5639702677726746, 0.39503684639930725, 0.6048223376274109, 0.7902161478996277, 0.018563969060778618, 0.5999511480331421, 0.6432732939720154, 0.4169047474861145, 0.6813555955886841, 0.9482240676879883, 0.8246023058891296, 0.623088002204895, 0.9651432633399963, 0.51152104139328, 0.5464646220207214, 0.490235835313797, 0.8671221733093262, 0.8909605145454407, 0.8644742369651794, 0.8114105463027954, 0.8722229599952698, 0.881458044052124, 0.8679150342941284, 0.7538665533065796, 0.9585999250411987, 0.8997929692268372, 0.7573737502098083, 0.9430184960365295, 0.8476705551147461, 0.8879924416542053, 0.7907237410545349, 0.8021093010902405, 0.6512376070022583, 0.8210548758506775, 0.8199359774589539, 0.7283215522766113, 0.8722819089889526, 0.7990031242370605, 0.929379940032959, 0.741117537021637, 0.8337192535400391, 0.7627476453781128, 0.8932207226753235, 0.48435673117637634, 0.8878312110900879, 0.7832436561584473, 0.7520095109939575, 0.8523473143577576, 0.7176631093025208, 0.8561399579048157, 0.847724199295044, 0.814816415309906, 0.8181338310241699, 0.7364667654037476, 0.748944103717804, 0.8115115165710449, 0.913548469543457, 0.8769396543502808, 0.7663840651512146, 0.8688212633132935, 0.7614914774894714, 0.635158360004425, 0.8190476298332214, 0.7032533288002014, 0.720851719379425, 0.7264574766159058, 0.7391771674156189, 0.8024022579193115, 0.7820784449577332, 0.9485514760017395, 0.8084370493888855, 0.8041379451751709, 0.9082171320915222, 0.8792212009429932, 0.5730867385864258, 0.8464133739471436, 0.6806883215904236, 0.5824285745620728, 0.8474743366241455, 0.8213481307029724, 0.8687213659286499, 0.7828460931777954, 0.7627865076065063, 0.8593116998672485, 0.7421143651008606, 0.9356016516685486, 0.7972131967544556, 0.8520556688308716, 0.8613623380661011, 0.8031009435653687, 0.8088792562484741, 0.7650240659713745, 0.8021162748336792, 0.857458233833313, 0.6476171612739563, 0.9695774912834167, 0.810535728931427, 0.7892048954963684, 0.824603796005249, 0.6095523238182068, 0.9292508959770203, 0.8000916242599487, 0.7976775765419006, 0.6723532676696777, 0.8641217350959778, 0.9241794347763062, 0.7039322853088379, 0.7947478890419006, 0.7463299632072449, 0.7008161544799805, 0.7622894048690796, 0.8830828666687012, 0.7935939431190491, 0.756841242313385, 0.8096973299980164, 0.8544514775276184, 0.7944643497467041, 0.7060107588768005, 0.8427478075027466, 0.8228822946548462, 0.6851910352706909, 0.7577974200248718, 0.7144296169281006, 0.865912675857544, 0.7435528635978699, 0.6791742444038391, 0.823733389377594, 0.658502459526062, 0.3153315484523773, 0.749081552028656, 0.9029482007026672, 0.8458194732666016, 0.8071855902671814, 0.7787235975265503, 0.8656759262084961, 0.8776628971099854, 0.8115970492362976, 0.8224440813064575, 0.8885993361473083, 0.6385905742645264, 0.7439751625061035, 0.8615444898605347, 0.8325061202049255, 0.8061923384666443, 0.7207509279251099, 0.636410117149353, 0.6891094446182251, 0.8291494250297546, 0.7114501595497131, 0.7760408520698547, 0.7454724311828613]

# precision = calculate_precision(iou_scores)
# recall = calculate_recall(iou_scores)
# map_score = calculate_map(iou_scores)
# f1_score = calculate_f1(precision, recall)

# print(f"Precision: {precision:.2f}")
# print(f"Recall: {recall:.2f}")
# print(f"mAP: {map_score:.2f}")
# print(f"F1-Score: {f1_score:.2f}")


# print(" ====== Faster results ==============")
# iou_scores = [0.687419056892395,
# 0.7109199166297913,
# 0.9373030066490173,
# 0.922423243522644,
# 0.892379641532898,
# 0.8101013898849487,
# 0.3536807596683502,
# 0.8816114068031311,
# 0.3747488856315613,
# 0.9472307562828064,
# 0.9608597159385681,
# 0.9353845119476318,
# 0.2529291808605194,
# 0.9387953281402588,
# 0.2848949134349823,
# 0.9865813255310059,
# 0.3163854479789734,
# 0.9675194025039673,
# 0.9547540545463562,
# 0.2813667356967926,
# 0.840568482875824,
# 0.8556550741195679,
# 0.03315993398427963,
# 0.3118831515312195,
# 0.9489901661872864,
# 0.8379836082458496,
# 0.9231352806091309,
# 0.8823195695877075,
# 0.0,
# 0.0,
# 0.3978685736656189,
# 0.792435348033905,
# 0.9247444868087769,
# 0.0,
# 0.0,
# 0.0,
# 0.9677625894546509,
# 0.0,
# 0.0,
# 0.0,
# 0.0,
# 0.9246802926063538,
# 0.329571396112442,
# 0.8443846106529236,
# 0.950834333896637,
# 0.9457365870475769,
# 0.8322941064834595,
# 0.9559482336044312,
# 0.5641031861305237,
# 0.23551848530769348,
# 0.2226615995168686,
# 0.932300329208374,
# 0.9376600384712219,
# 0.1875820755958557,
# 0.8608437776565552,
# 0.2644229233264923,
# 0.9534204006195068,
# 0.5520374178886414,
# 0.893368661403656,
# 0.4062000811100006,
# 0.9385693669319153,
# 0.9734144806861877,
# 0.9386391043663025,
# 0.9194807410240173,
# 0.2723078727722168,
# 0.23594613373279572,
# 0.22250691056251526,
# 0.9253703355789185,
# 0.9465538859367371,
# 0.9099127054214478,
# 0.9373189210891724,
# 0.9186204075813293,
# 0.9379764199256897,
# 0.9347781538963318,
# 0.9282509684562683,
# 0.9614102244377136,
# 0.9509623646736145,
# 0.9192413687705994,
# 0.9111724495887756,
# 0.976547360420227,
# 0.9457108378410339,
# 0.9536752104759216,
# 0.9509047269821167,
# 0.944239616394043,
# 0.9562597870826721,
# 0.9740386009216309,
# 0.9498077034950256,
# 0.9796050190925598,
# 0.9243533611297607,
# 0.9577798843383789,
# 0.928729236125946,
# 0.2984355390071869,
# 0.951055109500885,
# 0.4221333861351013,
# 0.9495328664779663,
# 0.23912671208381653,
# 0.30419355630874634,
# 0.9586380124092102,
# 0.9566982388496399,
# 0.2451183944940567,
# 0.9233458638191223,
# 0.2712334096431732,
# 0.41896533966064453,
# 0.9070772528648376,
# 0.566066563129425,
# 0.3372002840042114,
# 0.9433465600013733,
# 0.8825644850730896,
# 0.8388679027557373,
# 0.8784201145172119,
# 0.9794420599937439,
# 0.9338768124580383,
# 0.3336816430091858,
# 0.8926779627799988,
# 0.9079957604408264,
# 0.9590470790863037,
# 0.9024851322174072,
# 0.9200204610824585,
# 0.9315069913864136,
# 0.3613664209842682,
# 0.9053800106048584,
# 0.9130690097808838,
# 0.954912006855011,
# 0.8016386032104492,
# 0.9496713280677795,
# 0.9338688254356384,
# 0.3197541832923889,
# 0.862649142742157,
# 0.9598070979118347,
# 0.952363908290863,
# 0.9164332747459412,
# 0.9449852705001831,
# 0.885525107383728,
# 0.8935672044754028,
# 0.8524647951126099,
# 0.952409029006958,
# 0.9595280289649963,
# 0.9358642101287842,
# 0.935890793800354,
# 0.9046139717102051,
# 0.9262280464172363,
# 0.9655635356903076,
# 0.9313395023345947,
# 0.912266194820404,
# 0.7057557702064514,
# 0.8063057661056519,
# 0.9534387588500977,
# 0.9354613423347473,
# 0.5805497169494629,
# 0.7684503793716431,
# 0.9461090564727783,
# 0.426496684551239,
# 0.9542558789253235,
# 0.8955590724945068,
# 0.9463085532188416,
# 0.8410069346427917,
# 0.907106876373291,
# 0.25569239258766174,
# 0.9464925527572632,
# 0.24325864017009735,
# 0.0,
# 0.33379578590393066,
# 0.9626066088676453,
# 0.480571448802948,
# 0.9596527814865112,
# 0.9537580609321594,
# 0.31669485569000244,
# 0.416703999042511,
# 0.929244339466095,
# 0.23847392201423645,
# 0.9411963224411011,
# 0.9383416175842285,
# 0.9579380750656128,
# 0.23840564489364624,
# 0.9627697467803955,
# 0.5815045833587646,
# 0.9174547791481018,
# 0.9558224081993103,
# 0.8900573253631592,
# 0.9237274527549744,
# 0.9209643006324768,
# 0.8990932703018188,
# 0.9347643256187439,
# 0.8973857164382935,
# 0.6270459890365601,
# 0.7663911581039429,
# 0.3253358006477356,
# 0.9368234276771545,
# 0.9331076741218567,
# 0.5345648527145386,
# 0.8995880484580994,
# 0.7072309851646423,
# 0.885563850402832,
# 0.9326485395431519,
# 0.9562631249427795,
# 0.9663531184196472,
# 0.9443759322166443,
# 0.9412418007850647,
# 0.9580613970756531,
# 0.9281711578369141,
# 0.9447652697563171,
# 0.8895117044448853,
# 0.38441336154937744,
# 0.9389551877975464,
# 0.9497381448745728,
# 0.935543417930603,
# 0.9519569873809814,
# 0.950719952583313,
# 0.4225543737411499,
# 0.94711834192276,
# 0.2925650477409363,
# 0.26995643973350525,
# 0.947281539440155,
# 0.9624313712120056,
# 0.7933180928230286,
# 0.9729135632514954,
# 0.892619252204895,
# 0.9501030445098877,
# 0.9560789465904236,
# 0.2899182438850403,
# 0.9409374594688416,
# 0.9538119435310364,
# 0.9258021712303162,
# 0.9189879298210144,
# 0.9403220415115356,
# 0.9821249842643738,
# 0.8959004878997803,
# 0.9369282722473145,
# 0.9163696765899658,
# 0.9173981547355652,
# 0.9254045486450195,
# 0.9251760840415955,
# 0.9394690990447998,
# 0.9535291194915771,
# 0.8910321593284607,
# 0.9709364175796509,
# 0.8713464736938477,
# 0.9737376570701599,
# 0.9432165622711182,
# 0.8647711873054504,
# 0.8737929463386536,
# 0.9011816382408142,
# 0.9342490434646606,
# 0.9472513198852539,
# 0.8960134983062744,
# 0.9001104831695557,
# 0.9372746348381042,
# 0.8990007638931274,
# 0.9451114535331726,
# 0.9127206206321716,
# 0.22923757135868073,
# 0.0,
# 0.9432628750801086,
# 0.86969393491745,
# 0.9349202513694763,
# 0.9602534174919128,
# 0.0,
# 0.0,
# 0.0,
# 0.0,
# 0.0,
# 0.0,
# 0.6316606998443604,
# 0.0,
# 0.0,
# 0.0,
# 0.0,
# 0.0,
# 0.0,
# 0.0,
# 0.9489697813987732,
# 0.9676842093467712,
# 0.960851788520813,
# 0.957902729511261,
# 0.9635427594184875,
# 0.9464378356933594,
# 0.9140408635139465,
# 0.9670674800872803,
# 0.9402819871902466,
# 0.964025616645813,
# 0.9465711116790771,
# 0.9405751824378967,
# 0.948637843132019,
# 0.9355675578117371,
# 0.25905653834342957,
# 0.6308492422103882,
# 0.9201107025146484,
# 0.46127578616142273,
# 0.953432559967041,
# 0.9693043828010559,
# 0.9715508222579956,
# 0.39631521701812744,
# 0.923005223274231,
# 0.27798229455947876,
# 0.6692216396331787,
# 0.7025161385536194,
# 0.9540393948554993,
# 0.9707313776016235,
# 0.9719277024269104,
# 0.9178010821342468,
# 0.9615779519081116,
# 0.8457015752792358,
# 0.41064566373825073,
# 0.9434572458267212,
# 0.9134045839309692,
# 0.9426705241203308,
# 0.9201868176460266,
# 0.9452840089797974,
# 0.9355368614196777,
# 0.9283422231674194,
# 0.8942234516143799,
# 0.9352875351905823,
# 0.9088408946990967,
# 0.806556761264801,
# 0.9458330273628235,
# 0.5224187970161438,
# 0.8969091176986694,
# 0.9337552189826965,
# 0.8495426774024963,
# 0.8421661257743835,
# 0.8928889036178589,
# 0.9633271098136902,
# 0.7862107753753662,
# 0.8846684694290161,
# 0.8502857685089111,
# 0.8848913908004761,
# 0.326450914144516,
# 0.8560352921485901,
# 0.8442818522453308,
# 0.8807568550109863,
# 0.04760167375206947,
# 0.0,
# 0.9472742676734924,
# 0.9263978004455566,
# 0.8486102223396301,
# 0.015416624955832958,
# 0.9249277114868164,
# 0.0,
# 0.9483705163002014,
# 0.8650137186050415,
# 0.0,
# 0.8843119144439697,
# 0.0,
# 0.0,
# 0.0,
# 0.0,
# 0.0,
# 0.0,
# 0.6081390976905823,
# 0.937048077583313,
# 0.35713014006614685,
# 0.9503993988037109,
# 0.8519978523254395,
# 0.9299680590629578,
# 0.9827923774719238,
# 0.9316226243972778,
# 0.906994104385376,
# 0.0,
# 0.9666917324066162,
# 0.8554966449737549,
# 0.9271897673606873,
# 0.8798649311065674,
# 0.8990647196769714,
# 0.36728620529174805,
# 0.9397983551025391,
# 0.3029964566230774,
# 0.9419573545455933,
# 0.28414446115493774,
# 0.9638533592224121,
# 0.9790398478507996,
# 0.9562281966209412,
# 0.11596914380788803,
# 0.9348751902580261,
# 0.8745541572570801,
# 0.0023845594841986895,
# 0.020893409848213196,
# 0.0,
# 0.003721090732142329,
# 0.7809519171714783,
# 0.1268197000026703,
# 0.03742537274956703,
# 0.0,
# 0.0,
# 0.9439059495925903,
# 0.32768622040748596,
# 0.917313277721405,
# 0.8484913110733032,
# 0.9112085103988647,
# 0.939659059047699,
# 0.9450938105583191,
# 0.9144774079322815,
# 0.8875559568405151,
# 0.8980399966239929,
# 0.9525739550590515,
# 0.8735551238059998,
# 0.9425998330116272,
# 0.9566918611526489,
# 0.9129695892333984,
# 0.9475882649421692,
# 0.9637326002120972,
# 0.9303838014602661,
# 0.8778777122497559,
# 0.926811158657074,
# 0.9419267773628235,
# 0.8718118667602539,
# 0.9279814958572388,
# 0.9258965253829956,
# 0.5664299130439758,
# 0.941669762134552,
# 0.9637466073036194,
# 0.954086184501648,
# 0.23800601065158844,
# 0.8207979798316956,
# 0.4947792887687683,
# 0.9485466480255127,
# 0.7737717628479004,
# 0.8616135716438293,
# 0.9750966429710388,
# 0.923450231552124,
# 0.9738742113113403,
# 0.8999051451683044,
# 0.9506444931030273,
# 0.969903826713562,
# 0.9093267321586609,
# 0.9277860522270203,
# 0.9850825667381287,
# 0.9596496820449829,
# 0.9628814458847046,
# 0.9295908212661743,
# 0.902471661567688,
# 0.9213059544563293,
# 0.9175503253936768,
# 0.9230213165283203,
# 0.949058473110199,
# 0.8782058358192444,
# 0.9719796776771545,
# 0.9515422582626343,
# 0.8568048477172852,
# 0.97670978307724,
# 0.844301164150238,
# 0.9283145070075989,
# 0.9435513019561768,
# 0.5079211592674255,
# 0.9542202353477478,
# 0.9688488841056824,
# 0.9438562989234924,
# 0.8794308304786682,
# 0.9112634658813477,
# 0.9597933888435364,
# 0.9186742901802063,
# 0.9090762734413147,
# 0.9474688768386841,
# 0.9430942535400391,
# 0.9487090706825256,
# 0.9335265159606934,
# 0.9466842412948608,
# 0.8060576319694519,
# 0.9351728558540344,
# 0.9191542863845825,
# 0.9385093450546265,
# 0.9029374122619629,
# 0.0,
# 0.0,
# 0.0,
# 0.0,
# 0.0,
# 0.0,
# 0.9001771807670593,
# 0.0,
# 0.8916326761245728,
# 0.8708901405334473,
# 0.8797425627708435,
# 0.6942368149757385,
# 0.8087380528450012,
# 0.8693139553070068,
# 0.8809486031532288,
# 0.878226101398468,
# 0.768992006778717,
# 0.8599499464035034,
# 0.8440548181533813,
# 0.8179399967193604,
# 0.6501675844192505,
# 0.8486601114273071,
# 0.8215881586074829,
# 0.731564462184906,
# 0.9243802428245544,
# 0.8803187608718872,
# 0.94386887550354,
# 0.778545081615448,
# 0.0,
# 0.8566302061080933,
# 0.7638562917709351,
# 0.8706977367401123,
# 0.8603166341781616,
# 0.9160515666007996,
# 0.9329335689544678,
# 0.9048052430152893,
# 0.8811560273170471,
# 0.8158940672874451,
# 0.6258993744850159,
# 0.8851341605186462,
# 0.6359617114067078,
# 0.7779191732406616,
# 0.90095454454422,
# 0.7837107181549072,
# 0.8393730521202087,
# 0.7992526292800903,
# 0.8516502976417542,
# 0.8701391220092773,
# 0.8306726217269897,
# 0.892268180847168,
# 0.9158501625061035,
# 0.83161461353302,
# 0.8747934699058533,
# 0.923402726650238,
# 0.8102416396141052,
# 0.8337491154670715,
# 0.7985585927963257,
# 0.8992710113525391,
# 0.8113980889320374,
# 0.7602826356887817,
# 0.7527011036872864,
# 0.7086374163627625,
# 0.8748912215232849,
# 0.8248736262321472,
# 0.9111188054084778,
# 0.9052467346191406,
# 0.7331365942955017,
# 0.8614753484725952,
# 0.7722518444061279,
# 0.8651659488677979,
# 0.7452925443649292,
# 0.7620575428009033,
# 0.9215823411941528,
# 0.6142702102661133,
# 0.7899605631828308,
# 0.6423754692077637,
# 0.8371038436889648,
# 0.7331051826477051,
# 0.7432315349578857,
# 0.8576101660728455,
# 0.10099831223487854,
# 0.8495327234268188,
# 0.873760998249054,
# 0.894237220287323,
# 0.8183149099349976,
# 0.7578096985816956,
# 0.6903625130653381,
# 0.8601551651954651,
# 0.6962917447090149,
# 0.8656497597694397,
# 0.8419438600540161,
# 0.8052689433097839,
# 0.44147858023643494,
# 0.7214622497558594,
# 0.8655838370323181,
# 0.6910073757171631,
# 0.9129607081413269,
# 0.7486192584037781
# ]
# precision = calculate_precision(iou_scores)
# recall = calculate_recall(iou_scores)
# map_score = calculate_map(iou_scores)
# f1_score = calculate_f1(precision, recall)

# print(f"Precision: {precision:.2f}")
# print(f"Recall: {recall:.2f}")
# print(f"mAP: {map_score:.2f}")
# print(f"F1-Score: {f1_score:.2f}")
